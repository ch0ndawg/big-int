Project Big Int 
**************************************************************************************************

Summary: a big integer class supporting all the usual arithmetic operations and more. Why? for the hell of it, and solving a few long-standing childhood questions. I'm quite sure that big ints have been beaten to death and there's loads of available implmentations. But nevertheless this is an interesting model project to exercise the software-design and coding-practices muscles, and also a new testing ground for programming techniques. Grinding the programming axe, so to speak. Plus, dare I say it, it can be great fun!

**************************************************************************************************
Design & Implementation notes

• Big integers are represented by arrays of "digits," each being an unsigned char or unsigned short int. That is, each array member ranges from 0 to 255 or 0 to 65535, which is the "base" of the number system. Recall that we represent numbers in positional notation, that is, we only have 10 digits (unique symbols), and to represent numbers larger than 10, we move a place to the left. To do arithmetic operations, we fundamentally "memorize" the 1-digit-by-1-digit combinations. Here, the 256 or 65536 unique symbols are just bit-patterns. It happens that each "digit" has its sub-digits internal to the computer, but that doesn't interfere with our basic concept.

• Since the computer can already do arithmetic on chars and shorts, in order to implement arithmetic, fundamentally all we have to do is mimic the grade-school algorithms (carry, borrow, etc). Of course, since the computer supports arithmetic with integers larger than 255 or 65535, the intermediate result before a carry is also perfectly representable. Subtraction is a little bit more problematic because of the unsigned nature of the representation. Essentially, to borrow, we add an extra 255 onto things -- see notes on sign-and-magnitude. Addition/subtraction of a single-digit quantity is done differently, as it is faster. Multiplication and division are another story.

• Numbers are stored "little endian" which is "backward" from our usual representation (recall, however, that our representation was the right way for the Arabs), namely array element [0] is the units place, [1] is the 256's place, [2] is the 65536's place, and so on. This is because vector operations are most efficiently done at the back end, so in addition and multiplication, appending information to the back is apt to result in fewer reallocations (especially if we reserve memory beforehand). One exception to this is shifting during multiplication, but that is rendered irrelevant since we use the Fourier transform for that anyway. Multiplication and division by powers of 256 can be implemented with the correct shifts, which is what we do in the division algorithm (adding this special case was actually spurred by problems in getting division to work correctly for just these cases; we have to be on the lookout for these cases anyway, so why not just throw in the whole shift anyway? This could be the prelude to implementing a Big Float class). Also an important invariant we will maintain in the class is that leading zeros (i.e. zeros at the end of our representation) are trimmed after any operation (such as multiplication) that generates them. This ensures consistency in some algorithms, particularly comparison operations.

• We represent negative numbers as sign-and-magnitude, that is, have an extra field to note the sign. We chose '+' and '-' for this, of course, all our operations have to preserve the validity of those signs.

• Multiplication was initially chosen, as a first approximation, to be done the traditional way (at least to get the interface right). This brings up the design issue of what happens to intermediate results, since their size is bounded by the total number of digits in our big integer times the base, whereas in addition we're guaranteed that an individual digit sum never exceeds twice the maximum digit. The initial representation was that each digit be computed via the Cauchy product and a long double would hold the result -- good for a few millions of digits. However the Cauchy product is very slow and so we switched over to the FFT.

This still forces the choice of the data type of the intermediate results, which in turn depends on whether it's base 256 or 65536 we're considering. It turns out that double and long double both give comparable accuracy ranges for bases 256 and 65536 respectively, namely, good calculations up to numbers on the order of 3 million digits. With char (base 256) and long double, the limitations are pretty much the memory of the computer and the time on your hands. One of the most vexing things about long doubles, however, is that at least on this computer, it is 128 bits long, but only offers 80 bits of precision, which is better, but not overwhelmingly so, than the 64 such bits used in plain doubles. Floats are completely out; numbers of very modest size easily clobber it. The 128-bit long double slows things down by as much as a factor of 2 in large-scale calculations. We'll look into some issues surrounding this.

• Division is done via multiplication by the reciprocal, which is in turn computed, following the advice given in NR (Numerical Recipes), via Newton's method. This is a bit problematic because inversion is inherently a floating-point operation and we have to keep track of various shiftings. The Newton iteration is computed just far enough to allow an exact quotient and remainder (via the bi_ldiv_t structure). It is of course the most computationally intensive basic operation. However a separate routine is included for dividing by 1-digit quantities (called fastdiv) which does in fact replicate grade-school short-division. This capability is essential in radix conversion (which is by far the most complicated algorithm in our library so far).

• Added to the mix is calculation of positive integral powers (unsigned int is chosen as the type for the exponent). This calculation is very nifty, using recursion and squaring (namely, if it's odd, the answer is base * pow(base, (n-1)/2)^2 , and if it's even it is just pow(base, n/2)^2. Of course we have the induction-base-cases 0 (which gives 1, including for base 0), and 1, which gives the base itself.

• Other operations include: gcd (one of the prime uses for big ints in general--cryptography makes liberal use of it) and various C-family assignment, increment, and decrement operators. There is a private operation, ones complement, which is used to implement various things involving subtraction; however this is really an implementation detail, namely, dependent on the base-256 or 65536 nature of the beast.

• Input/Output: this is basically shorthand for: Radix conversion. If we want to output text in decimal digits, then we have to compute those decimal digits. There is the traditional method, which uses the fast-division routine: divide by 10, get remainder. Divide quotient by 10, get remainder. And so on; the remainders collected in reverse order are the digits.

But this is terribly slow (O(N^2), like Cauchy multiplication). It was found that single digit-by-digit radix conversion of a modest number of "only" a few thousand digits took 5+ minutes. The solution to this dilemma is yet another application of that ever-so-useful technique of recursion: first compute the digits in base 10^(1/2 the size of the number), using one big integer divide, this gives us two "digits" in base 10^(ridiculously large). But then each "digit" can be further subdivided into base 10^(less ridiculously large but still ridiculously large). We continue subdividing until we get to base 10^(large but not ridiculously so) and then apply the traditional radix conversion to each of these groups. The whole number is just the concatenation of those guys, since we're working in different powers of the same base. The major sticking point to watch out for here is placeholder zeros. If it's the first digit group, eliminate them; if they're in the middle, on the other hand, enough zeros must be filled in to ensure the correct size. More explanation is given in the source file itself.

• We take advantage of multithreading. The recursive nature of the radix conversion routine, including computing the two sides independently, obviously lends itself to parallel processing. So we researched multithreading and designed in a class that manages the multithreaded computation of this. The code is factored well enough that we can switch back to single-threaded operation if necessary (helps a lot in debugging), but eventually perhaps we should add some kind of immediately customizable options rather than having to recompile (template metaprogramming, perhaps?). The recursion itself is extremely amenable to multithreading; one simply converts the parameters into state variables in a function object. For radix conversion and my dual-core computer, multithreading (perhaps somewhat embarrasingly) only gives a 10% edge over the single-threaded, recursive algorithm, and as such, on single-processor machines, ought to be easily disabled. However, another area which found multithreading possibilities is the Fourier transform; redoing the FT on that also gives a "mere" 10% gain; however this does in fact make a big difference, in fact, in the radix conversion routine (the gain is about 50%). This is because that algorithm uses a lot of division, which in turn uses a lot of multiplication, so that 10% increase really compounds. More on this in "Interesting things learned." 

**************************************************************************************************

Interesting things learned:

• 6/30: Quick delegation of most of the raw data representation to STL structures. I've always wanted to learn the STL but never got around to it, and never really found a "real" project to apply STL to. As much as I pride myself on finally having fully understood dynamic allocation, why pointers matter, how to do it, and even how to roll my own version of it, and so on and so forth, at the end of the day, it's a real pain to do manually. Better just use system-provided facilites. However std::vector's allocator is in fact customizable, and is possibly very inefficient for small types like char and short,so we may revisit this arena eventually. But the consequence of this decision is that memory management is completely factored out of the big int class itself, so that I won't have to touch a thing (hopefully) if I later on decide to add the facility (which does in fact seem a promising proposition or a good idea following advice in Alexandrescu's book).

• 7/4: The FFT is clearly superior to Cauchy multiplication, even for small N. The issue of doubles vs. long doubles clearly turns out to be nontrivial; we'll have to settle perhaps with "only" millions of digits =). This forces a design choice between 256 and 65536 also. 65536 does speed things up a bit, even though it requires long double to function correctly, if only because perhaps the allocator is more efficient, or it is just in general easier to play around with digit strings half the size. Plus, we are relegating more arithmetic to the hardware (recall that if our numbers are less than 4 base-256 "digits", we're duplicating functionality already present in long ints). This should be a topic for further research.

• 7/6: Handling issues with subtraction (and just negative numbers) is, once again, surprisingly nontrivial, particularly with the sign-and-magnitude representation. The problem is, a different algorithm must be used for mismatched signs. For integers (positives and negatives), there are two different algorithms, and two operations called addition and subtraction, which do not coincide, except in the case that all numbers are positive and the minuend exceeds the subtrahend. The algorithmic process usually associated to "addition" when dealing with positive numbers is really used when the integers to be added have the same sign. So the sum of two negative integers uses the usual positive addition algorithm, and appends the sign later, and adding a negative number to a positive one really makes us use the classical subtraction algorithm.

• 7/8: Implementation of divide is highly nontrivial, as there are some annoying, nagging issues resulting in incorrect behavior, which I thought should not be difficult to handle, but are. Particularly problematic are degenerate or special cases, for which the Newton iteration performs differently, and the fact that we have to implicitly manage the radix point. There's a lot of questions about when does division yield a result which has a quotient strictly equal to the difference of the number of digits (leading zeros trimmed). Sometimes we have to make sure the zeros aren't trimmed prematurely.

• 7/10: Once division became stable, I went to work on the recursive radix conversion routine, which does division by humongous powers of 10. The algorithm is simple enough to define, and seemed quite straightforward... until the issue of handling placeholder zeros reared its head. That led to some intense debugging sessions. In essence, the problem is with programming is that things seem straightforward but turn out to not be so. Because the routine was recursive, this required messing around with what kind of parameters to pass to functions. Also this exposed a bug in the long division routine, although I would not discover its true nature until later (the kind of divisions performed in radix conversion was designed to involve a divisor of exactly at half the order of magnitude, which just so happened to be the minimum size divisor for which the algorithm succeeds).

• 7/12: Delighted by the success of the recursive radix conversion routine (there is no comparison of this algorithm to the single-digit-by-single-digit method, for sizable numbers; we're talking about speed improvements by factors of 10, 20 here), I set out on a more ambitious, monumental task: developing a parallel version of radix conversion. This was essentially a challenge I placed upon myself, since I had never written a nontrivial multithreaded program (the only thing I did do was some stupid trivial example in my Operating Systems class). First things first, to acquire a portable threading library; I chose the one contained in Boost, since the whole Boost project seems to have gotten some rave reviews by various C++ gurus.

Trouble surfaced as I couldn't get it to build for all the architectures my computer supports--In the interim I had discovered that this laptop in fact has a fully 64-bit processor and I was capable of running and emulating at least 4 distinct platforms on it. Looking at other choices I discovered also that the Pthreads library is actually fully embedded in the Mac OS X kernel itself (even native application/operating system threads are really Pthreads, on closer inspection by the various tools). I guess saying that Mac OS X has a Unix foundation does mean something more than being able to open up a terminal window and typing "ls."

This gave me a chance, however, to flex my abstraction muscles by attempting to encapsulate the threading model into C++ objects, so that I could switch threading models at will, via a mere recompile, rather than actual modification of the source code. I essentially implemented a bare-bones version of Boost threads by using objects with Pthreads inside them instead. Upon closer examination of the Boost sources, also, this is pretty much what they do too... except not so bare-bones.

• 7/13: After working through the basic features of Pthreads (and suitably encapsulating them in objects), I set out on actually redoing the recursive radix conversion. This required quite a bit of refactoring. Since the Boost interface calls for a function object which defines operator() which took no parameters and returned no value, I found myself having to transform my recursive function's parameters to instead hold state information as instance variables inside a function object (which the CS literature, somewhat bizarrely, refers to as a "closure"). But even then, it was not long before I came to understand the admonishments from the Boost.Thread documentation to be careful about passing the functor to the thread constructor by reference. I needed it to pass by reference, for the simple reason that the parent thread needed to access the result of the computation in order to assemble them into the final number. Passing by value would cause only a copy to be modified, leaving the bare original after returning. The thread would not modify its argument. Which I suppose would be fine if the thread was spun off to do something completely independent. Anyway the nasty undefined behavior rears its head when some function returns too soon, and hence the thread is modifying the data of some local variable which had long been cleaned up off the stack and destroyed. It took quite a bit of hunting down to pinpoint where this was happening (lesson learned: programming in the single-threaded frame of mind really does condition you to make assumptions about memory access that don't work in a multithreaded environment. It is impossible to return "too early" in a single-threaded program, because everything else is halted while the code is executing).

• 7/14: The first pass at multithreading the radix conversion has in fact resulted in an improvement in speed, albeit an embarrasingly small one. It is approximately 10% for large numbers. The good news is, it is easy enough to revert back to the single-threading model: simply call the function object rather than pass it to a thread constructor. The additional coordination, scheduling complexities, synchronization, and so forth, take their toll on performance, sapping nearly all of the advantage provided by an additional processor (core). I tested out various sizes of base cases (at which thread-spawning stops, and the radix conversion is done the ordinary way on this "sufficiently small" number), and it was easy to get worse performance for the multithreaded way. However a small improvement is still an improvement. 

A relatively easy fix brought another incremental speed increase, namely, the elimination of a redundant calculation during each recursion. Originally, at each step, I computed the power of 10 afresh each recursion. This is a waste of computational energy, and instead I arranged that before starting the recursion, I precompute all the requisite powers of 10, and insert them into a std::map, indexed by their power (an excellent opportunity again to make use of existing functionality in the STL). A std::map was chosen, of course, because it is a sparse array, that is, there is no need for all the powers of 10 in between those given in the algorithm. Retroactive testing shows our test case, 3^2^22, taking 8 minutes single-threaded, but only 7 minutes multithreaded. 8/7, or 14% is a small but significant improvement. With the redundant calculation I think it is 11/10; don't I wish I fixed Time Machine to work on this computer.

• 7/15: Of course, I did not fail to notice the remark in Numerical Recipes that it is possible to also make the Fourier Transform into a parallel algorithm. It was a little more difficult figuring out how I should implement this, since NR gives a recipe by allowing the dividing-up to be any power of 2. I made my choice to divide just into 2 pieces, almost paralleling the model established by the recursive radix conversion. I made this into a recursive algorithm, too. I have my doubts about whether this is the best way of doing things, especially since I haven't ever actually seen production-level implementation of parallel FFTs. What kinds of decisions go into appropriately breaking up the data to be FFT'd in smaller chunks? This is again some place I have tagged for further research. This method actually turned out to be fairly easy to implement once I understood the details of the algorithm worked, and once I'd made my choice of the correct division. The principal disadvantage as cited by the NR authors was that it required 3 transpose operations, although with my explicit choice (and knowledge of the triviality of a 2-element FFT) easily reduced it to just one transpose operation.

In fact the 2-element FFTs (which would be some higher power of 2, if some other choice is made, are supposed to be done in parallel, too, but I forwent that, since the operations are so trivial, anyway... it certainly would not justify the inherent overhead). I didn't check the speedup factor but I have reason to believe it isn't all that much (same order as what the multithreaded radix conversion had over original radix conversion). BUT, and this is a big but... the multithreaded radix conversion routine, which uses long division, which in turn uses large numbers of multiplications, of course, relies on the FFT, and multithreading the FFT does in fact result in a *significant* speedup in the radix conversion routine--those 10% savings for each multiplication *do* add up. Of course, if I did have access to a computer with a large number of processors, these routines would in fact be far superior, so there is no excuse to not program with parallelism in mind. The recursive FFT only kicks in when the array it operates on gets very large: 32768. This is much larger than the base case for radix conversion (primarily because radix conversion does a lot more work in the base case). For our test case, 3^2^22, this brings it down to 3 minutes, 10 seconds. This is more than twice as as fast as the ~7-minute execution time for single-threading the Fourier transform (but multithreading the radix conversion), which should be impossible, because I only have two processors on this computer (unless I unwittingly got another processor for free; after all, I can reliably get Activity Monitor to spike to 220%). I'm guessing it's because the reduced size of the data sets that the recursive FT operates on also represents a substantial gain in time efficiency. Hell, I was hoping for a smidgen over 10% improvement and I get something impossibly good... If so, a strong argument can perhaps be made even for single-processor computers to take advantage of multithreading, at least on the Fourier Transform.

• 7/16: I was so wrapped up in time-efficiency I hadn't stopped to think about the space efficiency. After all, everything in computer science is a tradeoff. What began as somewhat a tweak of the existing algorithm ballooned into a significant project on its own, because it reintroduced the whole early-return hazards as before. First, instead of storing the results in an instance variable in the function object itself (the "closure" version of a return value), we could instead try to insert the base-case strings into a top-level object. Of course this incurs some attendant synchronization issues. We insert each string into a std::map at the base case. Since the order of the computations is undefined (in theory they are all executed in parallel, remember), it's better to insert into the map where its location remains stable, rather than construct a vector of empty strings. The key for the std::map is the digit group ID number which starts off as 1 in the top-level thread, and shifts left, setting the last bit to 0 on the left digit's invocation, and 1 on the right's. This is not necessarily counting up from 0, and with elision of initial zeros, may not  start where it is expected to start. More importantly, the insertion of the result into a top-level map instead of storing it as an instance variable inside the currently executing function object allows the call to return immediately after starting its child threads, rather than waiting for the execution of the child threads to complete. This saves stack space in the threads.

Quite honestly I don't know how beneficial doing this really is, other than the fact I get this nice, cleaner feeling, knowing that each thread exits exactly when its work is done. However getting this to work correctly does result in some cost: I had to ensure that the quotient and remainder continued to be available to the child threads when the parent thread had returned (the parent threads' work is to do the long division to pass to the child threads; the threads at the bottom (base cases) are the ones that actually insert into the results map). This required, first, that its data be stored on the heap instead of the stack. And of course I had to store the references to these guys, along with the the pointers to the threads themselves, in the top level data structure which also held the results. Then the main thread simply joins with every thread in this structure, which is a queue based on lists. The standard std::queue adapter quickly fails because we start to join before all the threads have been added in the first place. Reallocations quickly invalidate the iterators involved, resulting in a glut of undefined behavior.

Ideally some sort of thread pool class should be used to handle the joining, but the one offered by Boost requires that the pool be full before starting the join process, while our threads continually spawn new ones; attempting this causes instant deadlock. So we're simultaneously joining and removing at the front, and inserting at the back; when the front catches up with the back, we are done. This is guaranteed to work, since the process of joining either ensures that a new thread or two is created, or that it is truly done, that is, outputted its digit string. However it doesn't quite "feel" right. Simultaneous insertion at the back and deletion at the front could conceivably be thread-unsafe in some STL implementations, because, say, some cached information or internal state is updated. The only guarantee that I know of from the Standard is that the iterators of the in-between elements will not be invalidated. Anyway better thread management in general is another thing on my list to learn... this is, recall, only my first real shot at it. There should be at the very least a limit on the number of threads created, for as it is, the threads proliferate, exponentially fast, completely seizing all of the system's resources.

• 7/17: The next big thing here was to get exception safety. What is especially interesting is trying to get it to work in the presence of multiple threads. This necessitated the use of a thread interruption function, which is at present only available in the Boost version of my thread abstraction class (I need to read up more on Pthreads in order to get this functionality in the Boostless version). This again proved to be a very interesting ride. I defined a general big int exception class, and had every thread object store a pointer to such a thing. If an exception propogates to the top level of the thread (caller of operator ()) then we clone the exception on the heap (the thread's stack is destroyed after it is rejoined, so we have to do this). Finally we pass it back to the join handling in our makeshift thread group class.

A very cute trick (whose validity which admittedly I am not entirely sure of) is that we can turn the exception back into its stack-based equivalent, that is, to repropagate it, by making a virtual function which throws itself! (throw *this). This is how we recover the dynamic type so it can again be caught by its true type in the main exception handling routine. Without this trick, we'd have to repropagate its pointer type and derived type information is completely lost. Not to mention that we must ask who has the responsibility of deleting the pointer. With repropagation, we take on this responsibility; to facilitate this, the trick is to retrieve the pointer from the returned (joined) thread object and store it in a std::auto_ptr; its virtual function which throws itself immediately causes its destructor to be called, thus properly deleting our object.

Of course there is a slight problem which does have an obvious resolution but so far I've been a bit lazy about implementing: how to handle standard exceptions, which are not derived from big_int's exception classes. Simple: we derive a standard exception wrapper from big_int's exception class, catch all standard exceptions in the top-level handler, and create one of these derived objects and stuff it in the exception field as before; the overridden version of repropagate() in this class, instead of throwing *this, throws the embedded std::exception. Because of the fact that standard exceptions do not have clone functions, we must use RTTI or something.

• 7/18: Of course, if an exception is thrown during computation, we should terminate any other threads. This requires us to use a notion of thread interruption, which fortunately is provided by Boost.Thread. This means, for now, the Pthreads version is defunct. This proved nontrivial; it forced me to look at the depth of the recursions and more closely at what kinds of exceptions may be thrown within threads. Several errors were caused by some return-too-soons because of exceptions generated deep into the FFT routines. The makeshift solution for now is to split the running of the thread and the exception handling into different function calls (i.e. operator()() calls run() and handles exceptions that run() emits) so that the behavior is different if the thread is started by a recursive call, or if it is invoked by the main thread. Thread-interrupted exceptions (due to cancellation) are ignored; this preserves the strong exception guarantee for radix conversion, since that process generates a new object (operates on const big_ints, remember). For the Fourier thread, only the basic guarantee is met, because the threads all modify its data vector in place. However, since the FFT is done on a copy of the data in the multiplication routine, this does not violate strong exception safety for multiplication (other than the fact that it does in fact reserve more memory for the vector, but this is not contrary to the logic of the program; I guess you could say it's a gray area, because it *could* in theory affect execution by causing out-of-memory errors where it would otherwise not happen.

• 7/21: Rebuilt the project as a shared library. I'd always wondered how to do this. It turns out to be very easy: just don't include the file that has main() in it. At this point I'll give it a rest for a few days.

• 7/26: I managed to take up the problem of matrix transposition: rather than making a copy of the vector and moving corresponding elements into place, I used cycle decompositions. Namely, interleaving the two halves of the vector is a very specific permutation of that vector, and hence amenable to permutation via cycle decompositions (cf. Knuth vol. 1), which uses only one temporary variable per cycle (it is in a sense a generalized swapping operation). It turns out that a cycle in the decomposition for interleaving has very interesting number-theoretic properties: the next element in the cycle is either half of it (if it is even), or it is half of its next congruence class modulo N-1, where N is the total number of elements to be interleaved, if the cycle element is odd. Of course, since N is even (otherwise we wouldn't be able to interleave at all!), we can always do the operation without introducing fractions.

The result? The Fourier operation now uses much less space (it still requires one array, of far smaller size but still proportional to N, to mark which elements in the cycle decomposition have been found, but it is now a vector<bool>, yes, the infamous vector<bool>, which is implemented as single bits). It also slightly improves the time (probably because it takes a while to copy all those doubles into another array), maybe by 2% or 3%. This is magnified again in our good old radix conversion algorithm, because of the vast number of multiplications it performs. For 3^2^22 we are down to 2:50, a savings of approximately 20 seconds (or a factor of 19/17). I still would like to see if we can eliminate the vector<bool>, that is, have an algorithm for deciding the start of a cycle without checking against a list. I managed to whittle it down to starting off with only odd numbers (since the last element before closing a cycle is exactly twice the starting number and so even numbers always occur in a cycle starting off odd) and only up N/2 (for N a power of 2, N= 2^k the cycles always have length at most k, since redoubling k times gives us 2^k x = (2^k -1)x + x, and the first term is 0 modulo 2^k -1; that is to say 2^(k-1) x closes the cycle).

7/30:

Finally implemented shifting-type operations in multiplication, too, so that multiplications by powers of 256 or 65536 are practically instantaneous (of course, if you want to do a radix conversion to display the number, that's a whole 'nother story).

===================================================================================================

Future Plans/Issues to be resolved
---------------------------------------------------------------------------------------------------

• 7/18:
	- Better thread and memory management via, perhaps circular-queueing
	- Exception safety for thread creation failures
	- Better optimized Fourier methods, perhaps completely different threading model
	- Optimization of the corresponding radix conversion for input: it's still the snail-slow digit-by-digit calculation we labored to banish from the output version over the past 2 weeks..
	- Pi calculation example (we would need to implement square root, but hopefully this can be done via the external big_int interface only, i.e. we need not implement it as a member or friend.

===================================================================================================

• 11/20/12

An attempt at resurrecting the project, which trailed off when the going got pretty tough with the PhD advising situation... Official student status came to an end, and much of the concurrency literature as well as just general programming resources I had, had to be returned... Life happens. Anyway, what brought this back, briefly, is that multithreading came back to the fore, simply because one of my programmer friends became immensely interested in programming for OpenCL. Needless to say, this would be extremely hard to adapt to OpenCL, and learning how to program the GPU is highly relevant for my now current thesis work. However, even this version appears to have become very broken: something broke in long division, and it may be related to broken STL implementations, iterators, and such. I recall having the hardest time getting that algorithm right, doing many test cases out and seemingly still getting random, inexplicable errors no matter how logical my fixes seemed. Finally, it just worked, with enough finessing here and there. However, I couldn't prove that it worked, or really understand why it was working, so it comes as no surprise that now it is broken again: something about the hacking to get it to work must have been dependent on the implementation of std::vectors (in either llvm or gcc).

Similarly, I tried reading over the old code and much of it is once again opaque. In particular, C++ templates really are a pain in the ass and really impede readability. I'd just been high off of reading Alexandrescu's book. It clearly is the wrong approach to doing various radix conversions (the algorithm in this project that I'm the most proud of, and actually the only part I can claim as original work). It would seem, anyway, that we should try to separate the interface and implementation more (at least it doesn't appear that it uses private data), and the templates really obfuscate everything.

=================================================================================================

• 12/15/15

Well, another very long interval has passed. The GPU computing/multithreading project never got off the ground. Principally, it was because I was trying very hard to get my start in GPU computing, in general, by enabling it for software that is close to my research: FEniCS, a finite element package. However, there were a number of half-baked configurations and poor documentation issues, which finally simply caused my computer to crash when attempting to compile shit to use the GPU. As a consequence, ever since getting this (very solid) MacBook pro with its immense GPU power, I have not managed to write anything at all that took advantage of the power. The time investment relative to my research goals eventually became not at all worth it, and I gave up that quest and put this aside.

At long last, I achieved the PhD, and significant pressure already is in place to get a job and finally actually, you know, start putting some of this to good use. What better activity, then, to resume programming, and learn concurrency in general, one of the things that has been on my computer science to-do list for a very long time. C++, meanwhile, has finally included some rudimentary concurrency utilities, so I can finally either dispense with boost, or commit to learning something in earnest. It seems that the notion of *futures* is the correct one for the radix conversion routine—basically, task-based programming rather than at the thread level. It properly automates the whole apparatus I had with shared memory, a place for return values, and exceptions. It's cool that I did get some form of approximation to the right thing, but of course, the professionals (who at the very least are probably much better at concurrency than I am) probably have it sorted out better than I do, so I'm off to rewrite the class using futures.

In addition, I finally (on paper, anyway) derived the proper inverse routine for the input, also done by recursion. Finally, a more formal algorithm analysis concludes firmly that the algorithm is O(N log N), or possibly O(N log N log log N), where N is the number of digits, whereas the standard algorithm is O(N^2). This of course was already borne out in experiments: the single-threaded, recursive algorithm absolutely kills compared to the traditional, even for numbers of "modest" size. Recursion is indeed the correct tool, because the depth of the recursion is only O(log N), and it is so highly amenable to parallelism (spinning each recursive call off in its own thread) that there really is no point in searching for the "right" iterative way of doing this: the stack frames/thread overhead are dwarfed in size by the actual data, even in just the base case (and in fact, that's probably what determines the size of the base case).

So what's next (after rewriting it to use futures)?

    - Try to get the GPU to do it.

    - Perhaps I can get the GPUs to at least do the base cases (because that would at least have compatible data type, and fastdiv is a lot easier to re-implement than general long division).

    - Speed up multiplication more via the FFT and the GPU; that'll really accelerate radix conversion, due to its reliance on long division and multiplication in many, many places. In addition

    - Lock-free and/or persistent data structures to store the temporary powers of 10 required to do huge long divisions. Perhaps a hash-map instead of a map (right now, it uses locking). Can you even do that in C++??

    - Adaptive code that handles the precision issue in the FFT mentioned above. Demystified the 128bit quad precision availability. Unfortunately, it is only available in software; long doubles in hardware are indelibly only 80 bits. Also, it must be compiled with GNU (although, I'm sure, there's probably a way of including it in llvm that I haven't figured out yet). Practically, this means anything using 128 bit precision is going to be sloooooowwwww.
